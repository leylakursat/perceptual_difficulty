---
title: "Models for Perceptual Difficulty Paper"
output:
  #html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r import, include=FALSE}
library(tidyverse)
library(lme4)
library(languageR)
library(brms)
library(lmerTest)
library(performance)

source("allFit.R")
source("helpers.R")

targets = read.csv("../data/targets.csv") #target selections in the production experiment
pd = read.csv(file="../data/exp3_agr_pd.csv") #response times to target's features in the perceptual difficulty experiment (in context)
pd_all = read.csv(file="../data/exp3_all_pd.csv")
```

# Exp.2
**Mixed effects logistic regression predicting redundant adjective use from fixed effects of redundant property, with random by-subject and by-item intercepts**

going from high difficulty-material redundant(0) to low difficulty-color redundant(1) & no redundancy(0) to redundancy(1) --> should be positive
```{r model1, echo=FALSE}
table(targets$trialType)
table(targets$redundant)
table(targets$trialType,targets$redundant)
# table(targets$gameid)
table(targets$targetName) # some targets appear in both high-difficulty and low-difficulty trials
table(targets$clickedobject) # multiple targets have the same object types 

#centering trialType 
targets = targets %>%
  mutate(trialType = as.factor(trialType)) %>%
  mutate(ctrialType = as.numeric(trialType) - mean(as.numeric(trialType)))

m = glmer(redundant ~ ctrialType + (1+ctrialType|gameid) + (1|targetName), data=targets, family="binomial")
summary(m)
```


# Exp.3
**Mixed effects linear regression predicting logRT to redundant adjective from fixed effects of redundant property --> to replicate the effect from Exp1**

going from high difficulty(0) to low difficulty = material to color adjectives --> logRT decreases = should be negative

```{r model2, echo=FALSE}
# JD: this analysis should be done in exp 3, not here. here you've just imported mean RTs by item. to do this analysis properly you need to do it on the RTs you got from exp 3 directly. part of the reason this won't converge is that there isn't any logRT variability by gameid (because nobody in exp 2 took the experiment that estimated RT, so you're just getting the exact same RT for each object regardless of gameid)
#m1 = lmer(logRT ~ ctrialType +  (1+ctrialType|gameid) + (1|targetName), data=tomodel,REML=F) 
#summary(m1)

# LK: should be fixed, pd_all has all raw RTs of exp2 (with exclusions)

# JD: OK, model throws non-convergence warnings, but it's one of the harmless ones. If you run it multiple times and it always returns the same results (which it does), we're fine. And if you use the "bobyqa" optimizer like below, it converges without warning and same values as with default optimizer. Clear effect of feature on response time in expected direction.
table(pd_all$featureQuestion)

pd_all = pd_all %>%
  mutate(featureQuestion = as.factor(featureQuestion)) %>%
  mutate(cFeatureQuestion = as.numeric(featureQuestion) - mean(as.numeric(featureQuestion))) %>%
  mutate(logRT = log(rt), rt_sec = rt/1000, workerid=as.factor(as.character(workerid)))

m1 = lmer(logRT ~ cFeatureQuestion +  (1+cFeatureQuestion|targetname) + (1+cFeatureQuestion|workerid), data=pd_all,REML=F, control=lmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e6))) 
summary(m1)
```

# Exp.2 & Exp. 3

**Mixed effects logistic regression predicting redundant adjective use from redundant property (color or material), RT to redundant adjective in context and their interaction**

bigger RT = more perceptually difficulty = less redundant adjective use
```{r model3, echo=FALSE}
pd_tomerge = pd %>% 
  mutate(redundantAdj = as.character(adjQuestion)) %>%
  mutate(targetName = targetname) %>%
  select(targetName,competitorName,notCompetitorName,redundantAdj,MeanRT) #62 rows (for each context we have RT for both color and material adj)




tomodel = targets %>%
  mutate(redundant = as.factor(as.character(redundant))) %>% # make sure that redundancy is stored as factor, not as integer
  mutate(redundantAdj = ifelse(trialType=="high_difficulty",as.character(clickedmaterial),as.character(clickedcolor))) %>%
  mutate(redundantAdj = ifelse(redundantAdj == "wood", "wooden", redundantAdj)) %>%
  left_join(pd_tomerge, by=c("targetName","competitorName","notCompetitorName","redundantAdj")) %>%
  mutate(logRT = log(MeanRT),MeanRT = MeanRT/1000) # JD: get log RT and code raw RTs in seconds instead of ms

# let's regress RT onto feature
m_res = lm(logRT ~ ctrialType, data=tomodel)
summary(m_res)

tomodel$reslogRT = residuals(m_res) # add RT residuals to main data frame

# correlation betewen RT and feature before residualizing
ggplot(tomodel, aes(x=trialType,y=logRT)) +
  geom_jitter(alpha=.5)

# correlation betewen RT and feature after residualizing
ggplot(tomodel, aes(x=trialType,y=reslogRT)) +
  geom_jitter(alpha=.5)

tomodel = tomodel %>%   
  mutate(cMeanRT = MeanRT-mean(MeanRT), clogRT = logRT-mean(logRT), creslogRT = reslogRT-mean(reslogRT))

ggplot(tomodel, aes(x=creslogRT,y=ctrialType,color=redundant)) +
  geom_point(alpha=.2)


m2 = glmer(redundant ~ ctrialType*creslogRT + (1+ctrialType*creslogRT|gameid) + (1|targetName), data=tomodel, family="binomial") 
# the problem that remains: the model can't converge (it does converge when you get rid of the by-gameid slope for tiral type, but that's not the right model) because (i think) it can't separate the variance contributed by the trial type and the mean rt predictor
summary(m2)
check_collinearity(m2) # low correlations, all good
```

```{r}
agr = tomodel %>% 
  mutate(redundant = as.numeric(as.character(redundant)), binreslogRT=cut_interval(reslogRT,10)) %>% 
  group_by(trialType,binreslogRT) %>% 
  summarise(Proportion = mean(redundant), CILow=ci.low(redundant),CIHigh=ci.high(redundant)) %>% 
  ungroup() %>% 
  mutate(YMin=Proportion-CILow,YMax=Proportion+CIHigh)

# no effect of RT above and beyond trial type:
ggplot(agr, aes(x=binreslogRT,y=Proportion,color=trialType,group=trialType)) +
  geom_point() +
  geom_smooth() +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25) +
  theme(axis.text.x=element_text(angle=45,hjust=1,vjust=1))

# histograms of mean logRTs by trial type completely disjoint. so: trial type explains difference in redundancy, but no added explanatory power from RT measure. next step: test whether relative RT explains outcome 
ggplot(tomodel, aes(x=logRT,fill=trialType)) +
  geom_histogram(alpha=.5)
```


**Same model with perceptual difficulty difference score for each context (difference between RTs to target's sufficient and redundant feature)**
```{r model5, echo=FALSE}
pd_wide = pd %>% 
  mutate(targetName = targetname) %>%
  select(targetName,competitorName,notCompetitorName,featureQuestion,MeanRT) %>%
  spread(featureQuestion,MeanRT) %>%
  rename(color_RT = color) %>%
  rename(material_RT = material)

tomodel = targets %>%
  mutate(redundantAdj = ifelse(trialType=="high_difficulty",as.character(clickedmaterial),as.character(clickedcolor))) %>%
  mutate(redundantAdj = ifelse(redundantAdj == "wood", "wooden", redundantAdj)) %>% 
  mutate(sufficientAdj = ifelse(trialType=="high_difficulty",as.character(clickedcolor),as.character(clickedmaterial))) %>%
  mutate(sufficientAdj = ifelse(sufficientAdj == "wood", "wooden", sufficientAdj)) %>% 
  left_join(pd_wide, by=c("targetName","competitorName","notCompetitorName")) %>%
  mutate(diffPd = ifelse(trialType=="high_difficulty",color_RT-material_RT,material_RT-color_RT))

m4 = glmer(redundant ~ diffPd + (1+trialType) + (1|gameid) + (1|targetName), data=tomodel, family="binomial")
summary(m4)
```
